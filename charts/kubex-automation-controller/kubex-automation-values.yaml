# ================================================================
# SECTION: CONNECTION PARAMETERS - COPY/PASTE FROM KUBEX UI
# Instructions:
# → Go to Kubex UI > Automation tab
# → Copy "Secure Connection Parameters" and paste below
# ================================================================

densify:
  url:
    host: <customerName>.densify.com # mandatory: <instance>.densify.com

densifyCredentials:
  username: '<username>' # This is the username to access Densify
  epassword: '<epassword>' # This is the encrypted password to access Densify

# ================================================================
# SECTION: VALKEY CONFIGURATION
# Instructions:
# → Configure credentials and storage for the Valkey cache
# → Uncomment storage.className if your cluster requires explicit storage class
# ================================================================

valkey:
  credentials:
    password: '<password>' # quote if includes special characters, cannot include SPACES
  # storage:
  #   className: <storageClassName> # Optional. Define if your environment requires it. 

  # Override Valkey resources based on Densify recommendations
#  resources: # Resource sizing (tune for your cluster/workload)
#    requests:
#      cpu: "250m"                                # Update based on Densify recommendations
#      memory: "256Mi"                            # Update based on Densify recommendations
#    limits:
#      memory: "512Mi"                              # Update based on Densify recommendations


# ================================================================
# SECTION: CLUSTER CONFIGURATION - MANUAL INPUT REQUIRED
# Instructions:
# → Manually configure based on your cluster and policies
# ================================================================

cluster:
  name: <clusterName>   # [REPLACE] Kubernetes cluster name

certmanager:
  enabled: false        # Set to true if using cert-manager

# ================================================================
# SCOPE CONFIGURATION
# Instructions:
# → Define which pods are eligible for automation using namespace and label filters
# → Each scope references a policy that defines automation behavior
# → Multiple scopes allow different automation rules for different parts of your cluster
#
# To add multiple scopes:
# 1. Copy the entire scope block below (from "- name:" to the end of "podLabels:")
# 2. Paste it below the existing scope and modify:
#    - Change the scope name to be unique
#    - Reference a different policy if desired
#    - Adjust namespace and label filters
#
# Example multiple scopes:
# scope:
#   - name: production-scope
#     policy: conservative-policy
#     namespaces: { operator: In, values: ["prod-*"] }
#   - name: development-scope  
#     policy: aggressive-policy
#     namespaces: { operator: In, values: ["dev-*"] }
# ================================================================
scope:
  - name: base-optimization-scope # Unique scope name
    policy: base-optimization     # Policy name (optional - uses defaultPolicy if omitted)
    namespaces:           # Mandatory - defines which namespaces to include/exclude
      operator: NotIn     # Options: In | NotIn
      values:
        - kubex           # Kubex namespace (exclude automation components)
    podLabels:            # Mandatory - defines which pods to include/exclude based on labels
      - key: <podLabelKey>        # Replace with actual label key (e.g., "app", "env", "tier")
        operator: In              # Options: In | NotIn  
        values:
          - <podLabelValue_1>     # Replace with actual label values
          - <podLabelValue_2>

# ================================================================
# SECTION: AUTOMATION POLICY - MANUAL CUSTOMIZATION ALLOWED
# Instructions:
# → Adjust automation and resizing behavior as needed
# → You can define multiple policies by duplicating the block under 'base-optimization'
#    - Change the policy name (e.g., 'dev-automation', 'prod-automation')
#    - Customize each policy's CPU/memory/resizing behavior separately
#    - Reference the desired policy by name in the 'scope.policy' field above
#
# Note: Some settings are included for forward compatibility and will be activated in future releases
# ================================================================

policy:
  # ================================================================
  # GLOBAL SETTINGS
  # ================================================================
  automationEnabled: true # Global Switch to enable/disable automation for the cluster
  defaultPolicy: base-optimization # Default policy used when scope.policy is not specified
  
  # ================================================================
  # AUTOMATION SCOPE CONTROL
  # Determines where automation inclusion/exclusion rules are defined
  # ================================================================
  remoteEnablement: false
    # false: Automation scope is controlled ONLY by this Helm configuration
    #        - Containers are included/excluded based on namespace and label selectors defined above
    #        - Kubex UI cannot override these automation decisions
    #        - Recommended for strict GitOps workflows and production environments
    #
    # true:  Automation scope is controlled by BOTH Helm configuration AND Densify UI
    #        - Base rules still apply from namespace/label selectors above
    #        - Customers can additionally enable/disable automation for specific containers via Densify UI
    #        - Provides flexibility for ad-hoc automation control without Helm updates
    #        - Useful for development environments and testing scenarios
  
  # ================================================================
  # POLICY DEFINITIONS
  # Instructions:
  # → If you wish to define multiple policies:
  #    - Copy the entire block under policies currently called 'base-optimization'.
  #    - Paste it below and give it a new, unique name (e.g., production-policy).
  #    - Reference the new policy name in the scope[].policy field in kubex-automation-values.yaml to apply it selectively.
  #
  # ⚠️  IMPORTANT: Policy names MUST follow RFC 1123 subdomain rules:
  #    ✅ Use: lowercase letters, numbers, hyphens, dots only
  #    ✅ Examples: base-optimization, dev-env, production.cpu
  #    ❌ Avoid: camelCase, snake_case, UPPERCASE, special characters
  #    Why: Policy names become webhook URLs (/mutate/{policy-name})
  # ================================================================
  policies: 
    base-optimization:
      allowedPodOwners: "Deployment,StatefulSet,CronJob,Rollout,Job,ReplicaSet,AnalysisRun"
      enablement:
        cpu:
          request:
            downsize: true
            upsize: true
            setFromUnspecified: false
          limit:
            downsize: false
            upsize: true
            setFromUnspecified: false
            unsetFromSpecified: false     # (Future release)
        memory:
          request:
            downsize: true
            upsize: true
            setFromUnspecified: false
          limit:
            downsize: false
            upsize: true
            setFromUnspecified: false
      inPlaceResize:                      # (Future release)
        enabled: false
      inPlaceResizeContainerRestart:      # (Future release)
        enabled: false
      podEviction:
        enabled: true
      safetyChecks:
        maxAnalysisAgeDays: 5             # Number of days before optimization is considered stale


# ================================================================
# DEPLOYMENT CONFIGURATION OVERRIDES (Optional)
# Uncomment and customize the values below to override default settings
# 
# ⚠️  IMPORTANT: For pod scan optimization in large clusters, see:
#    docs/Pod-Scan-Configuration.md for calculation formulas and pre-configured values
#    based on your cluster size and deployment phase.
# ================================================================
#deployment:
#  controllerEnv:  # Override controller environment variables (defaults in values.yaml)
#    debug: true                                  # Uncomment to enable debug mode for troubleshooting
#    podScanInterval: "6h45m"                     # How often to scan cluster for pods to check 
#    podScanTimeout: "6h30m"                      # Max time allowed for each pod scanning cycle  
#    podEvictionCooldownPeriod: "1m"              # Wait time between pod evictions (recommended default) 
#    recommendationsFetchInitialDelay: "1m"       # Startup delay before first recommendation fetch 
#    recommendationsFetchInterval: "1h"           # How often to refresh recommendations from API 
#    recommendationsResyncTimeout: "45m"          # Max time for recommendation sync operations 
#    recommendationDataFormat: "json"             # Storage format in Valkey: "json" (readable) or "protobuf" (compact)
#    nodeCpuHeadroom: "10%"                       # CPU headroom to reserve on each node (Can be in absolute value i.e. "100m" or in Percentage format i.e. "10%")
#    nodeMemoryHeadroom: "200Mi"                  # Memory headroom to reserve on each node (Can be in absolute value i.e. "200Mi" or in Percentage format i.e. "10%")
#    evictionThrottlingWindow: "6h"               # Time window for throttling pod evictions 
#    evictionThrottlingMax: "1000"                # Max number of pod evictions allowed within the time window 

  # ================================================================
  # RESOURCE OPTIMIZATION (Recommended)
  # Kubex cannot automate its own components - monitor them in Densify UI and manually optimize
  # ================================================================
  
  # Override webhook resources based on Densify recommendations
#  webhookResources: # Resource Specs for the webhook server pod
#    requests:
#      memory: "64Mi"                            # Update based on Densify recommendations
#      cpu: "100m"                                # Update based on Densify recommendations
#    limits:
#      memory: "512Mi"                            # Update based on Densify recommendations
  
  # Override gateway resources based on Densify recommendations
#  gatewayResources: # CPU and memory resource requests and limits for the pod
#    requests:
#      memory: "64Mi"                            # Update based on Densify recommendations
#      cpu: "100m"                                # Update based on Densify recommendations
#    limits:
#      memory: "512Mi"                            # Update based on Densify recommendations
  
  # Override controller resources based on Densify recommendations
#  controllerResources: # Resource Specs for the controller pod
#    requests:
#      memory: "128Mi"                            # Update based on Densify recommendations
#      cpu: "250m"                                # Update based on Densify recommendations
#    limits:
#      memory: "512Mi"                              # Update based on Densify recommendations



  # ================================================================
  # NODE SCHEDULING CONFIGURATION (Optional)
  # Uncomment and customize to control where controller and webhook pods are deployed
  # ================================================================
  
  # Node scheduling for controller deployment
#  controller:
#    nodeSelector:
#      node-pool: system                          # Example: target specific node pool
#    affinity:
#      nodeAffinity:
#        requiredDuringSchedulingIgnoredDuringExecution:
#          nodeSelectorTerms:
#          - matchExpressions:
#            - key: kubernetes.io/instance-type
#              operator: In
#              values: ["m5.large", "m5.xlarge"]
#    tolerations:
#    - key: "system-workloads"
#      operator: "Equal"
#      value: "true"
#      effect: "NoSchedule"
#    topologySpreadConstraints:
#    - maxSkew: 1
#      topologyKey: topology.kubernetes.io/zone
#      whenUnsatisfiable: DoNotSchedule
#      labelSelector:
#        matchLabels:
#          app: kubex-controller
  
  # Node scheduling for webhook deployment
#  webhook:
#    nodeSelector:
#      node-pool: system                          # Example: target specific node pool
#    affinity:
#      nodeAffinity:
#        preferredDuringSchedulingIgnoredDuringExecution:
#        - weight: 100
#          preference:
#            matchExpressions:
#            - key: node-type
#              operator: In
#              values: ["stable"]
#    tolerations:
#    - key: "system-workloads"
#      operator: "Equal"
#      value: "true"
#      effect: "NoSchedule"

